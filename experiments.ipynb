{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "743f6a1f-9f85-48a8-b117-c68d2787227e",
   "metadata": {
    "id": "743f6a1f-9f85-48a8-b117-c68d2787227e"
   },
   "source": [
    "# CS 6440: Introduction to Health Informatics\n",
    "## Experiment: LLM RAG-Based Question Answering from FHIR Data\n",
    "\n",
    "### Overview\n",
    "\n",
    "Welcome to the CS 6440 Introduction to Health Informatics class. This notebook is designed for students to experiment with using LLM RAG-based question answering from FHIR (Fast Healthcare Interoperability Resources) data. You will be extracting clinical notes from FHIR resources, loading the notes into a vector database (Chroma DB), and using a Language Learning Model (LLM) for inference to answer user queries.\n",
    "\n",
    "### Objectives\n",
    "\n",
    "- Extract clinical notes from FHIR resources.\n",
    "- Load extracted data into Chroma DB as a vector database.\n",
    "- Use the Google Flan-T5 model to answer specific clinical questions.\n",
    "- Compare the generated answers with pre-generated OpenAI GPT-3.5 Turbo model answers as ground truth.\n",
    "- Evaluate the quality of the answers provided by Flan-T5.\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "- Basic understanding of FHIR resources.\n",
    "- Familiarity with Python and Jupyter notebooks.\n",
    "- Basic knowledge of machine learning and natural language processing.\n",
    "\n",
    "### Patient Data\n",
    "\n",
    "We have provided 9 patient bundles in this notebook. These bundles contain the necessary FHIR resources needed for the experiment.\n",
    "\n",
    "### Questions to Answer\n",
    "\n",
    "Using the provided data, load it into the vector database and conduct the experiments to answer the following questions accurately:\n",
    "\n",
    "1. **What allergies does the patient have? Answer correctly.**\n",
    "2. **What medications is the patient currently taking? Answer correctly.**\n",
    "3. **What immunizations has the patient received? Answer correctly.**\n",
    "4. **What is the patient's primary diagnosis? Answer correctly.**\n",
    "5. **What procedures were conducted on the patient? Answer correctly.**\n",
    "6. **What treatments have been administered during the recent visit? Answer correctly.**\n",
    "7. **What is the patient's history of present illness? Answer correctly.**\n",
    "8. **What are the patient's vital info? Answer correctly.**\n",
    "9. **What findings are mentioned in the patient's assessment and plan? Answer correctly.**\n",
    "10. **What follow-up care is planned for the patient? Answer correctly.**\n",
    "\n",
    "### Instructions\n",
    "\n",
    "1. **Load FHIR Data:**\n",
    "   - Extract clinical notes from the provided patient bundles.\n",
    "   \n",
    "2. **Vector Database Setup:**\n",
    "   - Load the extracted clinical notes into Chroma DB as vectors.\n",
    "   \n",
    "3. **Model Inference:**\n",
    "   - Use the Google Flan-T5 model to infer answers to the questions based on the vectorized data.\n",
    "   \n",
    "4. **Evaluation:**\n",
    "   - Compare the inferred answers with the pre-generated OpenAI GPT-3.5 Turbo model answers.\n",
    "   - Evaluate the quality and accuracy of the Flan-T5 model's answers.\n",
    "\n",
    "### Disclaimer\n",
    "\n",
    "**Confidentiality Notice:** This document contains proprietary information and is intended solely for the use of students enrolled in the CS 6440 Introduction to Health Informatics class at Georgia Tech. Unauthorized use, dissemination, or sharing of this document is strictly prohibited.\n",
    "\n",
    "**Property Notice:** This document is the property of Georgia Tech and should be used exclusively for the purpose of this class.\n",
    "\n",
    "### Getting Started\n",
    "\n",
    "Follow the steps outlined in this notebook to complete the experiment. Ensure that you adhere to the confidentiality and property notices mentioned above.\n",
    "\n",
    "Let's begin by loading the necessary libraries and setting up the environment for the experiment.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192e8674-5c26-4c1b-aeec-c4eb27e173a9",
   "metadata": {
    "id": "192e8674-5c26-4c1b-aeec-c4eb27e173a9"
   },
   "outputs": [],
   "source": [
    "#!unzip Lab-5-FHIR-RAG-Integration.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f407041-5d44-4d0a-877a-f3b606fe6d56",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1f407041-5d44-4d0a-877a-f3b606fe6d56",
    "outputId": "d35faffe-e585-4f62-dbfa-6a4b4796307f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Install required dependecies and it might restart session which is ok. \n",
    "# Just run again after restart\n",
    "!pip install -r requirements_google_colab.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b94a9d5-b3a4-4f71-80b9-71e354ebdd6b",
   "metadata": {
    "id": "1b94a9d5-b3a4-4f71-80b9-71e354ebdd6b"
   },
   "source": [
    "## Please answer the following questions for task1_read_fhir_bundle:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a0a704-de6c-4c2a-ac50-646d67138135",
   "metadata": {
    "id": "00a0a704-de6c-4c2a-ac50-646d67138135"
   },
   "source": [
    "### *What is the bundle file name?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cf4a39-d6c1-4847-bdad-fce5bb979606",
   "metadata": {
    "id": "42cf4a39-d6c1-4847-bdad-fce5bb979606",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Specify the bundle file name containing the FHIR resources.\n",
    "# This is a placeholder; students should replace <Number> with the actual patient number they are working with.\n",
    "bundle_name = \"fhir_bundles/patient<Number>.json\"  # Placeholder, student should input their answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9559f862-9c9a-4276-a66f-d248f527a041",
   "metadata": {
    "id": "9559f862-9c9a-4276-a66f-d248f527a041"
   },
   "source": [
    "### *What is the bundle type?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f755ee-3e7e-4be0-891c-21d33313f1f5",
   "metadata": {
    "id": "63f755ee-3e7e-4be0-891c-21d33313f1f5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Specify the bundle type for processing the FHIR bundle.\n",
    "# This is a placeholder; students should replace <bundle_type> with the actual type they are using.\n",
    "transaction_type = \"<bundle_type>\"  # Placeholder, student should input their answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb950e3-ff8d-4189-a796-0d6311404c9d",
   "metadata": {
    "id": "bcb950e3-ff8d-4189-a796-0d6311404c9d"
   },
   "source": [
    "### *What is the resource type for clinical notes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2377d5f7-c42c-4b25-98c7-312ca6c011b8",
   "metadata": {
    "id": "2377d5f7-c42c-4b25-98c7-312ca6c011b8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Specify the resource type for extracting clinical notes from the FHIR bundle.\n",
    "# This is a placeholder; students should replace <resource_type> with the actual type they are using.\n",
    "resource_type = \"<resource_type>\"  # Placeholder, student should input their answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a85025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary functions from the helper files.\n",
    "# load_experiment_config: Function to load experiment configuration settings.\n",
    "# main_read_fhir_bundle: Function to read FHIR bundles and extract clinical notes.\n",
    "# main_vector_db_and_genai_model_config: Function to set up the vector database and configure the LLM model.\n",
    "# main_question_answering: Function to perform question answering using the vector database and LLM model.\n",
    "\n",
    "from experiment_helpers import load_experiment_config\n",
    "from experiments import main_read_fhir_bundle, main_vector_db_and_genai_model_config, main_question_answering\n",
    "\n",
    "# Define the path to the configuration file.\n",
    "config_path = 'experiments.json'\n",
    "\n",
    "# Load the initial experiment configuration from the specified JSON file.\n",
    "# The configuration file contains necessary parameters and settings for the experiment.\n",
    "config = load_experiment_config(config_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48a1a95-6e90-4afa-9b88-7e6d83d86b54",
   "metadata": {
    "id": "f48a1a95-6e90-4afa-9b88-7e6d83d86b54"
   },
   "source": [
    "# Update config for task1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744e1c18-535c-45f8-a64d-a4c979605201",
   "metadata": {
    "id": "744e1c18-535c-45f8-a64d-a4c979605201",
    "tags": []
   },
   "outputs": [],
   "source": [
    "config['tasks']['task1_read_fhir_bundle']['bundle_name'] = bundle_name\n",
    "config['tasks']['task1_read_fhir_bundle']['transaction_type'] = transaction_type\n",
    "config['tasks']['task1_read_fhir_bundle']['resource_type'] = resource_type\n",
    "config['tasks']['task1_read_fhir_bundle']['resource_attribute'] = \"attachment\"\n",
    "config['tasks']['task1_read_fhir_bundle']['additional_resource_type'] = \"DiagnosticReport\"\n",
    "config['tasks']['task1_read_fhir_bundle']['additional_resource_attribute'] = \"presentedForm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b811f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ§ª Task: Complete the required functions in experiments.py\n",
    "\n",
    "# You need to finish two functions marked with TODOs:\n",
    "\n",
    "# 1. decode_base64(data)\n",
    "#    - Find the line: \n",
    "#        # TODO: Replace None below with code to Decode the base64 string...\n",
    "\n",
    "# 2. extract_notes(...)\n",
    "#    - Find the line:\n",
    "#        # TODO: Replace None below with actual decoded function call\n",
    "\n",
    "# ðŸ“‚ If needed, check code_solution.py for the complete implementation.\n",
    "# â–¶ï¸ Next - Run main_read_fhir_bundle(config) to Read Clinical Notes from Patient JSON file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2c0738-1e4e-4cc5-a105-082739213392",
   "metadata": {
    "id": "8c2c0738-1e4e-4cc5-a105-082739213392",
    "tags": []
   },
   "source": [
    "## Extract Clinical notes from FHIR Bundle\n",
    "This will generate a text file with all clinical notes extracted from the FHIR Bundle.\n",
    "Please inspect the file and its content to understand how clinical notes are actually stored in FHIR.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fef1a04-31b9-463d-8650-952ec27de4d4",
   "metadata": {
    "id": "4fef1a04-31b9-463d-8650-952ec27de4d4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "main_read_fhir_bundle(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b425943-0e40-4afe-af8f-f97d4c954605",
   "metadata": {
    "id": "2b425943-0e40-4afe-af8f-f97d4c954605"
   },
   "source": [
    "## Please answer the following questions for task2_vector_db_and_genai_model_config:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6039f155-d13a-46c3-ad33-db93ca20532c",
   "metadata": {
    "id": "6039f155-d13a-46c3-ad33-db93ca20532c"
   },
   "source": [
    "### What is the embedding name?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701ad853-e47d-4b79-9a17-4bd2c6da7a0d",
   "metadata": {
    "id": "701ad853-e47d-4b79-9a17-4bd2c6da7a0d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "embedding_name = \"nomic-ai/nomic-embed-text-v1\"  # DO NOT CHANGE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efacd4a-6926-4e18-94df-d98afc41028c",
   "metadata": {
    "id": "3efacd4a-6926-4e18-94df-d98afc41028c"
   },
   "source": [
    "### What is the extracted clinical note text file name generated from task1?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec6b0fd-c5bb-425b-bb6f-2db1f55810ed",
   "metadata": {
    "id": "9ec6b0fd-c5bb-425b-bb6f-2db1f55810ed",
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_clinical_note_file = \"clinical_notes_output.txt\"  # DO NOT CHANGE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a50bad-b900-4c19-9a6e-5a4cbde9d8ab",
   "metadata": {
    "id": "63a50bad-b900-4c19-9a6e-5a4cbde9d8ab"
   },
   "source": [
    "### What is the base model name for this experiment?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10ab690-b155-4369-9981-e50870a7875d",
   "metadata": {
    "id": "e10ab690-b155-4369-9981-e50870a7875d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name = \"google/flan-t5-base\"  # DO NOT CHANGE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8599cc15-d369-49cf-ac94-ab8415bf586f",
   "metadata": {
    "id": "8599cc15-d369-49cf-ac94-ab8415bf586f"
   },
   "source": [
    "### Provide value temperature for Model\n",
    "\n",
    "### Temperature Attribute\n",
    "\n",
    "In the context of machine learning, particularly in the areas of transfer learning and model prediction, the temperature attribute is used to control the randomness of predictions. It is commonly applied in softmax functions to adjust the probability distribution of output classes. By modifying the temperature value, you can make the model's predictions either more confident (lower temperature) or more uncertain (higher temperature). Specifically:\n",
    "\n",
    "- **High Temperature**: The probabilities of different classes become more uniform, making the model's predictions less confident and more exploratory.\n",
    "- **Low Temperature**: The probabilities of the most likely classes become higher, making the model's predictions more confident and deterministic.\n",
    "\n",
    "This attribute is particularly useful in scenarios such as knowledge distillation, where the goal is to transfer knowledge from a larger, pre-trained model to a smaller one.\n",
    "\n",
    "In this experiment, we are using the Google flan-t5 model, and we have noticed that the temperature has very little impact on the outcome of the model. Therefore, you can ignore this parameter for this experiment and use the default value provided.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42ce4d1-44d3-4c83-94b5-c1f017de7ad2",
   "metadata": {
    "id": "a42ce4d1-44d3-4c83-94b5-c1f017de7ad2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "temperature = 0.3 # DO NOT CHANGE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba0f611-a378-4381-bb74-1711540caeb5",
   "metadata": {
    "id": "dba0f611-a378-4381-bb74-1711540caeb5"
   },
   "source": [
    "## Update config for task2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ad1898-7ca3-42cc-866b-404c26dee538",
   "metadata": {
    "id": "94ad1898-7ca3-42cc-866b-404c26dee538",
    "tags": []
   },
   "outputs": [],
   "source": [
    "config['tasks']['task2_vector_db_and_genai_model_config']['bundle_name'] = bundle_name\n",
    "config['tasks']['task2_vector_db_and_genai_model_config']['temperature'] = temperature\n",
    "config['tasks']['task2_vector_db_and_genai_model_config']['embedding_name'] = embedding_name\n",
    "config['tasks']['task2_vector_db_and_genai_model_config']['input_clinical_note_file'] = input_clinical_note_file\n",
    "config['tasks']['task2_vector_db_and_genai_model_config']['model_name'] = model_name\n",
    "#config['tasks']['task2_vector_db_and_genai_model_config']['fine_tuned_model_name'] = fine_tuned_model_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600a1808-cf77-4347-bffe-5a6053408a40",
   "metadata": {
    "id": "600a1808-cf77-4347-bffe-5a6053408a40"
   },
   "source": [
    "## Initialize Vector DB, Load Clinical Notes on Vector DB and Load the base and fine Tuned Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75424d1e-bf5d-42cc-bebd-1454403de882",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "75424d1e-bf5d-42cc-bebd-1454403de882",
    "outputId": "4ca5a92b-1d20-4259-ca1e-bcbc8c56e8d9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# If this step fails, please restart the session and run it again from What is the bundle file name? cell\n",
    "# Occasionally, this happens the first time due to Colab sync issues.\n",
    "main_vector_db_and_genai_model_config(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192485e8-ac86-4e69-a308-c677361db9d4",
   "metadata": {
    "id": "192485e8-ac86-4e69-a308-c677361db9d4"
   },
   "source": [
    "## Answer questions related Patient health"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71e2b10-3559-47fe-982c-2b6c33158b1a",
   "metadata": {
    "id": "f71e2b10-3559-47fe-982c-2b6c33158b1a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "main_question_answering(config, \"No\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef38ada-67c3-4796-aeb9-35ce180568c4",
   "metadata": {
    "id": "1ef38ada-67c3-4796-aeb9-35ce180568c4"
   },
   "source": [
    "## Evaluating Answer Quality Using NLP Metrics\n",
    "\n",
    "This section evaluates the quality of answers generated by the model (Flan-T5) compared to the reference answers (OpenAI). We will calculate common NLP metrics such as BLEU, ROUGE, METEOR, and BERTScore to assess the performance of the model. The results will be visualized using bar charts for each metric.\n",
    "\n",
    "### Steps:\n",
    "1. Load the JSON data from the `patient*_result.json` file.\n",
    "2. Initialize lists to store the BLEU, ROUGE, METEOR, and BERT scores for each question.\n",
    "3. Iterate through each question-answer pair in the JSON data.\n",
    "4. Calculate the BLEU, ROUGE, METEOR, and BERT scores for the model's answers compared to the reference (OpenAI) answers.\n",
    "5. Plot these scores in a bar chart for each metric.\n",
    "6. Save the evaluation results in a structured JSON file under the `submission/experiment*/experiment_result.json` directory.\n",
    "\n",
    "Run the following cell to perform these evaluations and visualize the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4434ce6-ee30-4892-a60f-0efc0436aeee",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 659
    },
    "id": "f4434ce6-ee30-4892-a60f-0efc0436aeee",
    "outputId": "91a6e992-41ae-40ff-81b3-1e14319879f4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "import json\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge import Rouge\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "import bert_score\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings about uninitialized weights and other specific warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"transformers.modeling_utils\")\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"huggingface_hub.file_download\")\n",
    "\n",
    "# Set up logging to suppress specific warning messages\n",
    "logging.getLogger(\"transformers.modeling_utils\").setLevel(logging.ERROR)\n",
    "\n",
    "# Define smoothing function and ROUGE\n",
    "smoothing_function = SmoothingFunction().method4\n",
    "rouge = Rouge()\n",
    "\n",
    "# Function to evaluate answers\n",
    "def evaluate_answers(student_data, openai_answers):\n",
    "    \"\"\"\n",
    "    Evaluate model answers against OpenAI answers using BLEU, ROUGE, METEOR, and BERTScore.\n",
    "\n",
    "    Args:\n",
    "        student_data (dict): The student file data.\n",
    "        openai_answers (dict): The OpenAI answers.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing questions, relative scores, detailed results, and error message (if any).\n",
    "    \"\"\"\n",
    "    relative_scores = []\n",
    "    questions = []\n",
    "    results = {}\n",
    "\n",
    "    if 'question_answering' not in student_data:\n",
    "        return None, None, None, \"No 'question_answering' key found in student data\"\n",
    "\n",
    "    for key, qa in student_data['question_answering'].items():\n",
    "        question = qa.get('question', 'No question provided')\n",
    "        model_answer = qa['model_answer']\n",
    "        openai_answer = openai_answers['question_answering'].get(key, {}).get('openai_answer', \"\")\n",
    "\n",
    "        if not openai_answer:\n",
    "            logging.warning(f\"No OpenAI answer found for question: {key}\")\n",
    "            continue\n",
    "\n",
    "        questions.append(key)\n",
    "\n",
    "        # Tokenize the answers\n",
    "        model_answer_tokens = model_answer.split()\n",
    "        openai_answer_tokens = openai_answer.split()\n",
    "\n",
    "        # Calculate BLEU Score\n",
    "        try:\n",
    "            bleu = sentence_bleu([openai_answer_tokens], model_answer_tokens, smoothing_function=smoothing_function)\n",
    "        except ZeroDivisionError:\n",
    "            bleu = 0.0\n",
    "            logging.warning(f\"BLEU score calculation failed for question: {key}\")\n",
    "\n",
    "        # Calculate ROUGE Score\n",
    "        try:\n",
    "            rouge_score = rouge.get_scores(' '.join(model_answer_tokens), ' '.join(openai_answer_tokens), avg=True)\n",
    "            rouge_l_f = rouge_score['rouge-l']['f']\n",
    "        except Exception as e:\n",
    "            rouge_l_f = 0.0\n",
    "            logging.warning(f\"ROUGE score calculation failed for question: {key} with error {str(e)}\")\n",
    "\n",
    "        # Calculate METEOR Score\n",
    "        try:\n",
    "            meteor = meteor_score([openai_answer_tokens], model_answer_tokens)\n",
    "        except Exception as e:\n",
    "            meteor = 0.0\n",
    "            logging.warning(f\"METEOR score calculation failed for question: {key} with error {str(e)}\")\n",
    "\n",
    "        # Calculate BERTScore\n",
    "        try:\n",
    "            P, R, F1 = bert_score.score([model_answer], [openai_answer], lang=\"en\", verbose=False)\n",
    "            bert_f1 = F1.mean().item()\n",
    "        except Exception as e:\n",
    "            bert_f1 = 0.0\n",
    "            logging.warning(f\"BERTScore calculation failed for question: {key} with error {str(e)}\")\n",
    "\n",
    "        # Logic to determine final score\n",
    "        if bert_f1 > 0:\n",
    "            if bleu > 0.1 or rouge_l_f > 0.1 or meteor > 0.1:\n",
    "                relative_score = bert_f1\n",
    "            else:\n",
    "                relative_score = bert_f1 * 0.3  # Reduce score if other scores are very low\n",
    "        else:\n",
    "            relative_score = 0.0\n",
    "\n",
    "        relative_scores.append(relative_score * 100)  # Convert to percentage\n",
    "\n",
    "        # Save individual results\n",
    "        results[key] = {\n",
    "            \"question\": question,\n",
    "            \"model_answer\": model_answer,\n",
    "            \"openai_answer\": openai_answer,\n",
    "            \"bleu_score\": bleu,\n",
    "            \"rouge_l_f_score\": rouge_l_f,\n",
    "            \"meteor_score\": meteor,\n",
    "            \"bert_score\": bert_f1,\n",
    "            \"final_percentage\": relative_score * 100\n",
    "        }\n",
    "\n",
    "    return questions, relative_scores, results, None\n",
    "\n",
    "# Extract patient number from bundle_name\n",
    "#bundle_name = \"fhir_bundles/patient2.json\"\n",
    "patient_number = bundle_name.split('/')[-1].replace('patient', '').replace('.json', '')\n",
    "student_file_path = f'patient{patient_number}_result.json'\n",
    "openai_answer_file_path = f'openai_answer/{patient_number}.json'\n",
    "\n",
    "try:\n",
    "    with open(student_file_path, 'r') as student_file, open(openai_answer_file_path, 'r') as openai_answer_file:\n",
    "        student_data = json.load(student_file)\n",
    "        openai_answers = json.load(openai_answer_file)\n",
    "\n",
    "        questions, relative_scores, results, error = evaluate_answers(student_data, openai_answers)\n",
    "\n",
    "        if error:\n",
    "            print(f\"Error in file {student_file_path}: {error}\")\n",
    "        else:\n",
    "            # Plot the metrics\n",
    "            fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "            bars = ax.bar(questions, relative_scores, color='skyblue')\n",
    "            ax.set_title('Relative Performance Scores Compared to OpenAI Answers')\n",
    "            ax.set_xlabel('Questions')\n",
    "            ax.set_ylabel('Relative Performance (%)')\n",
    "            ax.set_xticks(range(len(questions)))  # Set the tick locations\n",
    "            ax.set_xticklabels(questions, rotation=45)  # Set the tick labels\n",
    "\n",
    "            # Add numeric values on top of the bars\n",
    "            for bar in bars:\n",
    "                height = bar.get_height()\n",
    "                ax.annotate(f'{height:.1f}%',\n",
    "                            xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                            xytext=(0, 3),  # 3 points vertical offset\n",
    "                            textcoords=\"offset points\",\n",
    "                            ha='center', va='bottom')\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "            # Save results to JSON file\n",
    "            experiment_dir = f'submission/experiment{patient_number}'\n",
    "            os.makedirs(experiment_dir, exist_ok=True)\n",
    "            experiment_result_path = os.path.join(experiment_dir, 'experiment_result.json')\n",
    "            with open(experiment_result_path, 'w') as experiment_result_file:\n",
    "                json.dump(results, experiment_result_file, indent=4)\n",
    "            print(f\"Results saved to {experiment_result_path}\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    logging.error(f\"File not found: {e.filename}\")\n",
    "except json.JSONDecodeError as e:\n",
    "    logging.error(f\"Error decoding JSON: {str(e)}\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"An unexpected error occurred: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f065bce-354a-4833-98df-4914a249052f",
   "metadata": {
    "id": "9f065bce-354a-4833-98df-4914a249052f"
   },
   "source": [
    "# Verify Experiments\n",
    "\n",
    "This step will be successfully completed only when you complete experiments with 9 FHIR Resource Bundle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e588de14-5378-41cd-b9c8-1ad0c215f4b1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e588de14-5378-41cd-b9c8-1ad0c215f4b1",
    "outputId": "636d6131-4a25-4a3a-b594-e6d98fc87e55"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "EXPERIMENT_PATH = './submission'\n",
    "RESULTS_PATH = './results'\n",
    "\n",
    "TOLERANCE = 0.05\n",
    "\n",
    "def read_json_content(path):\n",
    "    try:\n",
    "        with open(path, 'r') as f:\n",
    "            return json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        return None\n",
    "\n",
    "def write_json_content(path, content):\n",
    "    with open(path, 'w') as f:\n",
    "        json.dump(content, f, indent=4)\n",
    "\n",
    "def check_values(obj):\n",
    "    if isinstance(obj, dict):\n",
    "        for key, value in obj.items():\n",
    "            if key != \"openai_answer\":  # Skip the optional openai_answer attribute\n",
    "                check_values(value)\n",
    "    elif isinstance(obj, list):\n",
    "        for item in obj:\n",
    "            check_values(item)\n",
    "    else:\n",
    "        assert obj is not None and obj != \"\", \"Found empty or None value\"\n",
    "\n",
    "def _test_patient_results(experiment_num):\n",
    "    # Build the paths to the JSON files in both directories\n",
    "    submission_file = os.path.join(EXPERIMENT_PATH, f\"experiment{experiment_num}\", \"experiment_result.json\")\n",
    "    result_file = os.path.join(RESULTS_PATH, f\"experiment{experiment_num}\", \"experiment_result.json\")\n",
    "\n",
    "    # Load the JSON content from both files\n",
    "    submission_content = read_json_content(submission_file)\n",
    "    result_content = read_json_content(result_file)\n",
    "\n",
    "    # If any of the files are not found, skip the test with a user-friendly message\n",
    "    if submission_content is None:\n",
    "        print(f\"Experiment {experiment_num} skipped: Submission result file '{submission_file}' not found.\")\n",
    "        return False\n",
    "\n",
    "    if result_content is None:\n",
    "        print(f\"Experiment {experiment_num} skipped: Result file '{result_file}' not found.\")\n",
    "        return False\n",
    "\n",
    "    # Check the scores for each question in the experiment result\n",
    "    for question_key, question_data in submission_content.items():\n",
    "        result_question_data = result_content.get(question_key, None)\n",
    "\n",
    "        if not result_question_data:\n",
    "            print(f\"Result for {question_key} not found in experiment {experiment_num}\")\n",
    "            return False\n",
    "\n",
    "        for metric in [\"bleu_score\", \"rouge_l_f_score\", \"meteor_score\", \"bert_score\"]:\n",
    "            student_score = question_data.get(metric, 0)\n",
    "            result_score = result_question_data.get(metric, 0)\n",
    "\n",
    "            # Ignore comparisons if either score is zero\n",
    "            if student_score == 0 or result_score == 0:\n",
    "                continue\n",
    "\n",
    "            if student_score < result_score - TOLERANCE:\n",
    "                print(f\"{metric} for {question_key} in experiment {experiment_num} is below the threshold. Expected >= {result_score - TOLERANCE}, got {student_score}\")\n",
    "                return False\n",
    "\n",
    "    print(f\"Experiment {experiment_num} passed.\")\n",
    "    return True\n",
    "\n",
    "# Run the tests manually\n",
    "required_tests = [1, 4, 8]\n",
    "tests_passed = {num: False for num in required_tests}\n",
    "\n",
    "for experiment_num in required_tests:\n",
    "    if _test_patient_results(experiment_num):\n",
    "        tests_passed[experiment_num] = True\n",
    "\n",
    "# Check if all required tests passed\n",
    "all_required_tests_passed = all(tests_passed.values())\n",
    "\n",
    "# Update submission.json if all required tests passed\n",
    "submission_file = os.path.join(\".\", 'submission.json')\n",
    "submission_content = read_json_content(submission_file)\n",
    "if submission_content is not None:\n",
    "    if all_required_tests_passed:\n",
    "        submission_content['state'] = '1'\n",
    "        print(\"Required tests passed. Updating 'submission.json' state to 1.\")\n",
    "    else:\n",
    "        print(\"Not all required tests passed. 'submission.json' state remains unchanged.\")\n",
    "    write_json_content(submission_file, submission_content)\n",
    "else:\n",
    "    print(\"'submission.json' file not found.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954d14a5-a00e-41ff-8542-65096110a776",
   "metadata": {
    "id": "954d14a5-a00e-41ff-8542-65096110a776"
   },
   "source": [
    "# Prepare Submission\n",
    "\n",
    "Once you completed all mandatory experiments run this step to prepare final submission file for Gradescope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a6accb-1857-4184-8856-c4b043ad4be6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a0a6accb-1857-4184-8856-c4b043ad4be6",
    "outputId": "52bf4fe0-c59e-4919-9b87-fec684d65f3a"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "EXPERIMENT_PATH = './submission'\n",
    "SUBMISSION_JSON_PATH = os.path.join(\".\", 'submission.json')\n",
    "ZIP_PATH = './submission.zip'\n",
    "\n",
    "def read_json_content(path):\n",
    "    try:\n",
    "        with open(path, 'r') as f:\n",
    "            return json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        return None\n",
    "\n",
    "def zip_submission_folder():\n",
    "    if os.path.exists(ZIP_PATH):\n",
    "        os.remove(ZIP_PATH)  # Remove existing zip file if it exists\n",
    "    shutil.make_archive(EXPERIMENT_PATH, 'zip', EXPERIMENT_PATH)\n",
    "    print(f\"Submission folder zipped successfully into {ZIP_PATH}.\")\n",
    "\n",
    "# Check the state of submission.json\n",
    "submission_content = read_json_content(SUBMISSION_JSON_PATH)\n",
    "if submission_content is not None:\n",
    "    if submission_content.get('state') == '1':\n",
    "        zip_submission_folder()\n",
    "    else:\n",
    "        print(\"'submission.json' state is not '1'. No zipping performed.\")\n",
    "else:\n",
    "    print(\"'submission.json' file not found.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f760f2-ee8a-40fc-ae3e-dc20d64eb3c3",
   "metadata": {
    "id": "45f760f2-ee8a-40fc-ae3e-dc20d64eb3c3"
   },
   "source": [
    "# Optional:\n",
    "## Use your Own OpenAI Key to check OpenAI Answer\n",
    "\n",
    "Please make sure if you want execute this step successfully you set openai key in file openai_key.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e5016c-de10-434d-9e71-e2837aed5a1b",
   "metadata": {
    "id": "02e5016c-de10-434d-9e71-e2837aed5a1b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from experiment_helpers import add_openai_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc95718-036e-423d-b2d9-10c9f6e4a3e7",
   "metadata": {
    "id": "ebc95718-036e-423d-b2d9-10c9f6e4a3e7",
    "tags": []
   },
   "outputs": [],
   "source": [
    "add_openai_answers(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5530439-6ea9-4349-a310-37364ba0ddbb",
   "metadata": {
    "id": "b5530439-6ea9-4349-a310-37364ba0ddbb"
   },
   "source": [
    "# Optional:\n",
    "## Experiments on Zero-Shot Prompt Engineering\n",
    "\n",
    "We have given few example. Try various prompts and see how model providing better answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fcabd7d-21c3-4333-8062-f0d12072bf6a",
   "metadata": {
    "id": "6fcabd7d-21c3-4333-8062-f0d12072bf6a"
   },
   "outputs": [],
   "source": [
    "question = \"What are the patient's vital info? Answer correctly\"\n",
    "context = \"\"\"\n",
    "Assessment and Plan Patient is presenting with medication review due (situation), full-time employment (finding), stress (finding). Plan Patient was given the following immunizations: influenza, seasonal, injectable, preservative free. The following procedures were conducted: - assessment of health and social care needs (procedure) - assessment of anxiety (procedure) - screening for domestic abuse (procedure) - assessment of substance use (procedure) - assessment using alcohol use disorders identification test - consumption (procedure) End of Clinical Note, Date of Clinical note Date: 2021-05-03 02:55:50 Doctor: Dr. Leontine92 Schmeler639 Organization: BROCKTON HOSPITAL, INC. 2021-05-03 Chief Complaint No complaints.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "t5_prompt = f\"\"\"\n",
    "\n",
    "[Context]: {context}\\n\\n\n",
    "\n",
    "[Question]: {question}\\n\\n\n",
    "[Answer]:\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from experiments import flan_t5_tokenizer, flan_t5_model\n",
    "input_ids = flan_t5_tokenizer(t5_prompt, return_tensors='pt').input_ids.to(\"cuda\")\n",
    "outputs = flan_t5_model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_new_tokens=50\n",
    "    )\n",
    "answer = flan_t5_tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {answer}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b349df3-e063-4319-b8f0-a718e6e3b85c",
   "metadata": {
    "id": "4b349df3-e063-4319-b8f0-a718e6e3b85c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "question = \"What are the medications is the patient taking? Answer correctly\"\n",
    "context = \"\"\"\n",
    "Medications No Active Medications. Assessment and Plan Patient is presenting with medication review due (situation), not in labor force (finding), stress (finding). Plan Patient was given the following immunizations: influenza, seasonal, injectable, preservative free. The following procedures were conducted: - medication reconciliation (procedure) - assessment of health and social care needs (procedure) - assessment of anxiety (procedure) - depression screening (procedure) - depression screening using patient health questionnaire two-item score (procedure) - assessment of substance use (procedure) - assessment using alcohol use disorders identification test - consumption (procedure) End of Clinical Note, Date of Clinical note Date: 2010-06-14 02:55:50 Doctor: Dr. Malka46 Gerhold939 Organization: LAKESIDE FAMILY PRACTICE 2010-06-14 Chief Complaint No complaints. History of Present Illness Cornell131 is a 31 year-old nonhispanic white male.\n",
    "\"\"\"\n",
    "\n",
    "#t5_prompt = f\"[Context]: {context}\\n\\n [Question]: {question}\\n\\n[Answer]:\"\n",
    "\n",
    "t5_prompt = f\"\"\"\n",
    "\n",
    "[Context]: {context}\\n\\n\n",
    "\n",
    "[Question]: {question}\\n\\n\n",
    "[Answer]:\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from experiments import flan_t5_tokenizer, flan_t5_model\n",
    "input_ids = flan_t5_tokenizer(t5_prompt, return_tensors='pt').input_ids.to(\"cuda\")\n",
    "outputs = flan_t5_model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_new_tokens=50\n",
    "    )\n",
    "answer = flan_t5_tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {answer}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eec39a0-756b-4464-aec7-01fb556fb50f",
   "metadata": {
    "id": "4eec39a0-756b-4464-aec7-01fb556fb50f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "question = \"What immunizations has the patient received? Answer correctly\"\n",
    "context = \"\"\"\n",
    "Allergies No Known Allergies. Medications acetaminophen 21.7 mg/ml / dextromethorphan hydrobromide 1 mg/ml / doxylamine succinate 0.417 mg/ml oral solution Assessment and Plan Plan Patient was given the following immunizations: influenza, seasonal, injectable, preservative free, hep a, adult. The following procedures were conducted: - assessment of health and social care needs (procedure) - depression screening (procedure) - depression screening using patient health questionnaire two-item score (procedure) - assessment of substance use (procedure) - screening for drug abuse (procedure) End of Clinical Note, Date of Clinical note Date: 2017-03-13 02:55:50 Doctor: Dr. Leontine92 Schmeler639 Organization: BROCKTON HOSPITAL, INC. 2017-03-13 Chief Complaint No complaints.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "#t5_prompt = f\"[Context]: {context}\\n\\n [Question]: {question}\\n\\n[Answer]:\"\n",
    "\n",
    "t5_prompt = f\"\"\"\n",
    "\n",
    "[Context]: {context}\\n\\n\n",
    "[Instruction]: Answer the following question based on the provided context. If the information cannot be found in the context, respond with \"No information found.\"\n",
    "\n",
    "[Question]: {question}\\n\\n\n",
    "[Answer]:\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from experiments import flan_t5_tokenizer, flan_t5_model\n",
    "input_ids = flan_t5_tokenizer(t5_prompt, return_tensors='pt').input_ids.to(\"cuda\")\n",
    "outputs = flan_t5_model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_new_tokens=50\n",
    "    )\n",
    "answer = flan_t5_tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {answer}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff2df0c-3929-4441-b43f-ed94e54fd5d5",
   "metadata": {
    "id": "3ff2df0c-3929-4441-b43f-ed94e54fd5d5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "question = \"What is the patient's primary diagnosis? Answer correctly\"\n",
    "context = \"\"\"\n",
    "Assessment and Plan Patient is presenting with medication review due (situation), full-time employment (finding), stress (finding). Plan Patient was given the following immunizations: influenza, seasonal, injectable, preservative free. The following procedures were conducted: - assessment of health and social care needs (procedure) - assessment of anxiety (procedure) - screening for domestic abuse (procedure) - assessment of substance use (procedure) - assessment using alcohol use disorders identification test - consumption (procedure) End of Clinical Note, Date of Clinical note Date: 2021-05-03 02:55:50 Doctor: Dr. Leontine92 Schmeler639 Organization: BROCKTON HOSPITAL, INC. 2021-05-03 Chief Complaint No complaints.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "t5_prompt = f\"\"\"\n",
    "\n",
    "[Context]: {context}\\n\\n\n",
    "\n",
    "[Question]: {question}\\n\\n\n",
    "[Answer]:\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from experiments import flan_t5_tokenizer, flan_t5_model\n",
    "input_ids = flan_t5_tokenizer(t5_prompt, return_tensors='pt').input_ids.to(\"cuda\")\n",
    "outputs = flan_t5_model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_new_tokens=50\n",
    "    )\n",
    "answer = flan_t5_tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {answer}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069028e0-9ff9-4490-ae3b-4a14e2f018d6",
   "metadata": {
    "id": "069028e0-9ff9-4490-ae3b-4a14e2f018d6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "question = \"What is the patient's history of present illness? Answer correctly\"\n",
    "context = \"\"\"\n",
    "Assessment and Plan Patient is presenting with medication review due (situation), full-time employment (finding), limited social contact (finding). Plan Patient was given the following immunizations: influenza, seasonal, injectable, preservative free. The following procedures were conducted: - assessment of health and social care needs (procedure) - assessment of anxiety (procedure) - screening for domestic abuse (procedure) - depression screening (procedure) - depression screening using patient health questionnaire two-item score (procedure) End of Clinical Note, Date of Clinical note Date: 2017-03-02 20:55:50 Doctor: Dr. Leontine92 Schmeler639 Organization: BROCKTON HOSPITAL, INC. 2017-03-02 Chief Complaint No complaints. History of Present Illness Cornell131 is a 37 year-old nonhispanic white male. Patient has a history of part-time employment (finding), medication review due (situation), not in labor force (finding), stress (finding).\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "t5_prompt = f\"\"\"\n",
    "\n",
    "[Context]: {context}\\n\\n\n",
    "\n",
    "[Question]: {question}\\n\\n\n",
    "[Answer]:\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from experiments import flan_t5_tokenizer, flan_t5_model\n",
    "input_ids = flan_t5_tokenizer(t5_prompt, return_tensors='pt').input_ids.to(\"cuda\")\n",
    "outputs = flan_t5_model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_new_tokens=50\n",
    "    )\n",
    "answer = flan_t5_tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {answer}\\n\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
